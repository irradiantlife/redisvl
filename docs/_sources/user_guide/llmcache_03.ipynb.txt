{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Caching\n",
    "\n",
    "RedisVL provides an ``SemanticCache`` interface to turn Redis into a semantic cache to store responses to previously asked questions. This reduces the number of requests and tokens sent to the Large Language Models (LLM) service, decreasing costs and enhancing application throughput (by reducing the time taken to generate responses).\n",
    "\n",
    "This notebook will go over how to use Redis as a Semantic Cache for your applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will import [OpenAI](https://platform.openai.com) to use their API for responding to user prompts. We will also create a simple `ask_openai` helper method to assist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import getpass\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\"\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\") or getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "openai.api_key = api_key\n",
    "\n",
    "def ask_openai(question: str) -> str:\n",
    "    response = openai.Completion.create(\n",
    "      engine=\"text-davinci-003\",\n",
    "      prompt=question,\n",
    "      max_tokens=200\n",
    "    )\n",
    "    return response.choices[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris.\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "print(ask_openai(\"What is the capital of France?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing and using ``SemanticCache``\n",
    "\n",
    "``SemanticCache`` will automatically create an index within Redis upon initialization for the semantic cache content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redisvl.llmcache import SemanticCache\n",
    "\n",
    "llmcache = SemanticCache(\n",
    "    name=\"llmcache\",                     # underlying search index name\n",
    "    prefix=\"llmcache:item\",              # redis key prefix\n",
    "    redis_url=\"redis://localhost:6379\",  # redis connection url string\n",
    "    distance_threshold=0.1               # semantic distance threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Index Information:\n",
      "╭──────────────┬────────────────┬───────────────────┬─────────────────┬────────────╮\n",
      "│ Index Name   │ Storage Type   │ Prefixes          │ Index Options   │   Indexing │\n",
      "├──────────────┼────────────────┼───────────────────┼─────────────────┼────────────┤\n",
      "│ llmcache     │ HASH           │ ['llmcache:item'] │ []              │          0 │\n",
      "╰──────────────┴────────────────┴───────────────────┴─────────────────┴────────────╯\n",
      "Index Fields:\n",
      "╭───────────────┬───────────────┬────────╮\n",
      "│ Name          │ Attribute     │ Type   │\n",
      "├───────────────┼───────────────┼────────┤\n",
      "│ prompt_vector │ prompt_vector │ VECTOR │\n",
      "╰───────────────┴───────────────┴────────╯\n"
     ]
    }
   ],
   "source": [
    "# look at the index specification created for the semantic cache lookup\n",
    "!rvl index info -i llmcache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the capital of France?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the cache -- should be empty\n",
    "llmcache.check(prompt=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache the question and answer\n",
    "llmcache.store(prompt=question, response=\"Paris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'response': 'Paris', 'vector_distance': '8.34465026855e-07'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the cache again to see if new answer is there\n",
    "llmcache.check(prompt=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'response': 'Paris',\n",
       "  'prompt': 'What is the capital of France?',\n",
       "  'vector_distance': '8.34465026855e-07'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update the return fields to gather other kinds of information about the cached entity\n",
    "llmcache.check(prompt=question, return_fields=[\"response\", \"prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'response': 'Paris', 'vector_distance': '0.0988066792488'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for a semantically similar result\n",
    "llmcache.check(prompt=\"What actually is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Widen the semantic distance threshold\n",
    "llmcache.set_threshold(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'response': 'Paris', 'vector_distance': '0.273138523102'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Really try to trick it by asking around the point\n",
    "# But is able to slip just under our new threshold\n",
    "llmcache.check(\n",
    "    prompt=\"What is the capital city of the country in Europe that also has a city named Nice?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invalidate the cache completely by clearing it out\n",
    "llmcache.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# should be empty now\n",
    "llmcache.check(prompt=question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance\n",
    "\n",
    "Next, we will measure the speedup obtained by using ``SemanticCache``. We will use the ``time`` module to measure the time taken to generate responses with and without ``SemanticCache``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def answer_question(question: str) -> str:\n",
    "    \"\"\"Helper function to answer a simple question using OpenAI with a wrapper\n",
    "    check for the answer in the semantic cache first.\n",
    "\n",
    "    Args:\n",
    "        question (str): User input question.\n",
    "\n",
    "    Returns:\n",
    "        str: Response.\n",
    "    \"\"\"\n",
    "    results = llmcache.check(prompt=question)\n",
    "    if results:\n",
    "        return results[0][\"response\"]\n",
    "    else:\n",
    "        answer = ask_openai(question)\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without caching, a call to openAI to answer this simple question took 0.32796525955200195 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# asking a question -- openai response time\n",
    "answer = answer_question(\"What is the capital of France?\")\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Without caching, a call to openAI to answer this simple question took {end-start} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "llmcache.store(prompt=\"What is the capital of France?\", response=\"Paris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken with cache enabled: 0.07565498352050781\n",
      "Percentage of time saved: 76.93%\n"
     ]
    }
   ],
   "source": [
    "cached_start = time.time()\n",
    "cached_answer = answer_question(\"What is the capital of France?\")\n",
    "cached_end = time.time()\n",
    "print(f\"Time Taken with cache enabled: {cached_end - cached_start}\")\n",
    "print(f\"Percentage of time saved: {round(((end - start) - (cached_end - cached_start)) / (end - start) * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistics:\n",
      "╭─────────────────────────────┬─────────────╮\n",
      "│ Stat Key                    │ Value       │\n",
      "├─────────────────────────────┼─────────────┤\n",
      "│ num_docs                    │ 1           │\n",
      "│ num_terms                   │ 0           │\n",
      "│ max_doc_id                  │ 2           │\n",
      "│ num_records                 │ 2           │\n",
      "│ percent_indexed             │ 1           │\n",
      "│ hash_indexing_failures      │ 0           │\n",
      "│ number_of_uses              │ 12          │\n",
      "│ bytes_per_record_avg        │ 0           │\n",
      "│ doc_table_size_mb           │ 0.000139236 │\n",
      "│ inverted_sz_mb              │ 0           │\n",
      "│ key_table_size_mb           │ 2.76566e-05 │\n",
      "│ offset_bits_per_record_avg  │ nan         │\n",
      "│ offset_vectors_sz_mb        │ 0           │\n",
      "│ offsets_per_term_avg        │ 0           │\n",
      "│ records_per_doc_avg         │ 2           │\n",
      "│ sortable_values_size_mb     │ 0           │\n",
      "│ total_indexing_time         │ 0.514       │\n",
      "│ total_inverted_index_blocks │ 0           │\n",
      "│ vector_index_sz_mb          │ 3.0161      │\n",
      "╰─────────────────────────────┴─────────────╯\n"
     ]
    }
   ],
   "source": [
    "# check the stats of the index\n",
    "!rvl stats -i llmcache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the cache\n",
    "llmcache.clear()\n",
    "\n",
    "# Remove the underlying index\n",
    "llmcache._index.delete(drop=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rvl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
